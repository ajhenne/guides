#+title: Radio Data
#+STARTUP: overview

* LINC

The software for processing LOFAR data.

Pipeline documents: [[https://linc.readthedocs.io/en/latest/]]
LOFAR LTA: https://lta.lofar.eu

** Installation

LINC is the pipeline for processing LOFAR data.

Download LINC workflow files or use the already downloaded files on ALICE.
~git clone https://git.astron.nl/RD/LiNC.git <install dir>~
~/data/grbemission/shared/LINC/~

Next, obtaining Singularity container containing all the actual pipeline scripts. (Singularity is now called apptainer but runs effectively the same)

Installing apptainer isn't reqiured on Leicester HPC, as it's already a provided module. You don't need to load this before running the pipeline as this command can be included in the job submission script.
~module load apptainer~

There are two methods of running LINC - you can run cwltool which fetches the container as part of the pipeline, or get the container first and run cwltool within it. I've had more success with the latter option. In this case, cwltool is part of the container so no additional installation is required. If you'd like to try the other way, no neeed to pull the container manually, and installing cwltool is a simple pip install. The running commands are slightly different so check the LINC documents.

Pull the container from Astron or use the container already stored on ALICE.
~apptainer pull docker://astronrd/linc~
~/data/grbemission/shared/linc_latest.sif~

** Downloading data

You will first need to have an account on LOFAR LTA - inquire with Antonia or just email the LTA team yourself (if I remember correctly they'll have to contact Rhaana or Antonia anyway as PI's to add you to the unpublic projects).

Once this is done, you can go to the website > browse projects and select the cycle, and then the specific project you're interested in. Select show data > averaging pipeline to access the datasets. The names aren't so descriptive at this point, but you can usually easily identify the calibrator and target measurements. For the case of GRBs, you can select 'All Dataproducts' and look under 'Start Time' to get the date and hence identify which GRB it is.

Once you know what data you want, click the top checkbox to select all datasets and then click 'stage selected' (top centre above table). You'll get a confirmation with filesize - click submit. You'll get an email confirming the data is queued which can take some hours to send. The data has to be moved from tape to disk so this step, depending on other user activity and data amount, can take some hours to a few days. Once the data is ready, you'll recieve an email.

Within the email there will be a .txt file, a list of paths to each measurement set that'll you'll download, copy the (contents) of the file to a .txt file in ALICE, where you want the data to be downloaded to. Before downloading you'll need to add credentials so the LTA knows it's you when you start requesting the files.

Create a file at ~~/.wgetrc~ and add to it, filling in your LTA details:
#+BEGIN_SRC bash
username=<lta_username>
password=<lta_password>
#+END_SRC

Now you are ready to download. The bottleneck is usually somewhere on ASTRON's end, but can just be UOL's wifi being busy, so this stage can take a fair amount of time. This can cause issues with sessions timing out if you do this through SSH. You could submit a long job request with low memory, but this can take time just for the job to actually start. Instead I prefer to use NoMachine (look at ALICE document pages for installation help) because these sessions don't time out for 3 days.

Open two terminals, one for calibrator and target, and open each to the directory in scratch where you want to download the respective data to. Then download:
~wget -ci lta_file.txt~

The c flag means that it'll carry on and resume from where it stopped in the case the download process is halted, the i flag just means it'll download files from the list within the .txt file. If you get 401 Authorization errors (more than once) then your LTA credentials aren't working.

*** Preparing data

Once done, the downloaded sets will be .tar files with long names. There is an ASTRON provided script to unzip and clean this up.
~python /data/grbemission/shared/linc_files/scripts/cleanupfiles.py .~

Run this when you're in the directory with the tar files and it'll clean this up to just include the observation code and measurement number.

** Running

With the data downloaded, you're ready to run the pipeline. The general outline at least for GRBs is to run the calibration pipeline on the calibrator data, take the output solutions and run the target pipeline on the target data with these solutions.

First create an input JSON file pointing towards all the measurement sets. At this stage you can include additional pipeline options as part of this file, details can be found in the documentation, but for a simple run nothing else is reqiured. There's another script provided that can do this automatically, unless you fancy manually typing out 244 filepaths.

I usually run this from the base directory for that specific projects, i.e. if I'm in /scratch/grbemission/ah724/GRBXXXXXXA, I set <directory> to /uncalibrated_data/calib - change as you prefer, just be aware that this path needs to correct relative to where you run the pipeline job from.

I also send the output straight into a json file, and then remove the couple of output lines the script prints.
#+BEGIN_SRC bash
python /data/grbemission/shared/linc_files/scripts/GetMSlist.py <directory> > input_calibrator.json

# the output into the piped will produce something similar to:
{
    "msin": [
        {"class": "Directory", "path": "/path/to/data_01.MS"},
        {"class": "Directory", 'path": "/path/to/data_02.MS"},
        ...
    ]
}
#+END_SRC

If you haven't separated the calib and target datasets into separate directories, that's ok, just be sure to include a wildcard selecting only the correct ones when you add <directory>. The first run should just be calibrator data.

*** Running LINC pipeline

An example command to run LINC is included, and then below a more full example of submitting the job to ALICE.

#+BEGIN_src bash
apptainer exec --bind /scratch:/scratch
<path/to/container.sif>
cwltool \
--outdir /path/to/cwd/outdir/ \
--logdir /path/to/cwd/logdir/ \
--preserve-entire-environment \
--parallel \
--no-container \
~/data/grbemission/shared/linc_latest.sif~
<path_to_cwl_files>/workflows/HBA_calibrator.cwl \
<input.json>
#+END_src

Assuming this is run from the project base directory, then input.json is also stored here pointing /uncalibrated_data/calib in my case. 'outdir' is the directory to save the finished diagnostic plots and solutions (e.g. ~/calibration_pipeline/outdir/~), 'logdir' contains all the logs for each step of the pipeline (e.g. ~/calibration_pipeline/logdir/~), and the other commands are for running within the singularity container. 'parallel' allows for parallel processes to happen and significantly speeds up the pipeline.

<path_to_container.sif> is the path to the container, either where you installed it or using the shared location:  ~/data/grbemission/shared/linc_latest.sif~
<path_to_cwl_files> is the path to your downloaded workflow files, alternatively if you're using the ones stored in ALICE already then set this to: ~/data/grbemission/shared/LINC/~.
<input.json> is simply the name of your JSON file.

Be sure to include the bind statement otherwise apptainer can't access scratch and the pipeline will fail.

Depending on the size of the data files and speed of processing, each pipeline can take 3-7 hours for GRBs, closer to 7 if using debugging options (see LINC documentation).

When running on ALICE by default a whole process log file is created automatically.

**** Full Leicester submission script

A full example script is included below. Change the job name, memory, time, mail and account as required. I prefer to define the LINC and work directories above just so I can change thees a bit easier when I run on different datasets.

#+BEGIN_src bash
#!/bin/bash

#SBATCH --job_name=cal_200416a
#SBATCH --nodes=1
#SBATCH --cpus-per-task=16
#SBATCH --mem=128gb
#SBATCH --time=08:00:00
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=ah724@leicester.ac.uk
#SBATCH --export=Non
#SBATCH --account=grbemission

module load apptainer

export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
export LINC_DIR=/data/grbemission/shared/LINC
export WORK_DIR=/scratch/grbemission/ah724/LOFAR_Followup/GRB200416A
cd $WORK_DIR

apptainer exec --bind /scratch:/scratch \
    $LINC_DIR/linc_latest.sif \
    cwltool \
        --outdir "/calibration_pipeline/outdir" \
        --logdir "/calibration_pipeline/logdir" \
        --preserve-entire-environment \
        --parallel \
        --no-container \
        $LINC_DIR/workflows/HBA_calibrator.cwl \
        input_calib.json
#+END_src



**** Target pipeline
The process is mostly the same. The JSON file now needs to give a list of the target files. In the submission script, ~HBA_calibrator.cwl~ should now be ~HBA_target.cwl~.
When you create the target JSON file, you will also need to add a line to point the pipeline to the calibration solutions, as such:

#+BEGIN_EXAMPLE
{
    "msin": [
        {"class": "Directory", "path": "/path/to/data_01.MS"},
        {"class": "Directory", 'path": "/path/to/data_02.MS"},
        ...
        ],
    "cal_solutions": {"class": "File", "path": "calibration_pipeline/outdir/cal_solutions.h5"}
}
#+END_EXAMPLE


*** Debugging

Include these options for debugging.
#+BEGIN_src bash
--debug
--tmpdir-prefix /path/to/cwd/tempdir/
--preserve-entire-environment
--leave-tmpdir
#+END_src

** POSSIBLE ISSUES

Just things that came up as potential issues for me. These may just be user error, specific problems with the datasets, or things that have since been fixed by ASTRON. Most common error will be the structure_function issue.

*** Failing on download_target_skymodel
*** Wrong download URL
Edit $LINCDIR/scripts/download_skymodel_target.py
Line 120 - change URL ~gsmv4~ to ~gsmv5~
*** A different wrong download URL?
Finds an error 302 on normal URL with cgi, then tries and fails on same URL but acgi.
Checking the python files I can't find any reference to acgi - it should just true cgi x5 then fail if this isn't valid but instead tries cgi then acgi 5 times.
In this case, just follow the actual URL to the page. Create file target.skymodel somewhere in the directory, and add to target.json:
~"target_skymodel": {"class": "File", "path": "/path/to/target.skymodel"}~

*** Failing on target/structure_function
Usually due to flagged data making some or all bands band, or totally deleted.
To .json add ~'make_structure_function': false~


* WSClean

Software is included in the LINC Singularity container - this needs to be active to run WSClean.

** Basic Imaging Command

#+BEGIN_src bash
wsclean \
-mgain 0.8 \           # Cleaning parameters.
-auto-mask 10 \
-pol I \
-maxuv-l 8000 \
-auto-threshold 3 \
-weight briggs -0.5 \
-niter 100000 \
-weighting-rank-filter 3.0 \
-fit-beam \
-reorder \
-clean-border 0 \
-apply-primary-beam \
-join-channels \
-no-update-model-required \
-name <name> \        # User parameters.
-channels-out 6 \
-size 2048 2048 \
-scale 1asec \
*.ms | tee imaging.log
#+END_src

Timeslicing
#+BEGIN_SRC bash
-reorder --> -no-reorder
-intervals-out X            # Split observation into X chunks.
-interval A B               # Only use slices A to B of the whole dataset, splitting it into X chunks.
#+END_src

** msoverview

~msoverview in=file.ms (verbose=T)~
View detailed information about the measurement sets. I believe this command is part of CASA, or in the Singularity container.

** Output
Produces primary beam images, dirty, model, individual beam visibilities for each outputted timeslice and per frequency channel requested.

*-image-pb.fits are the files most interesting to us.

Example Recent Run
#+BEGIN_src bash
wsclean \
-mgain 0.8 \
-auto-mask 10 \
-pol I \
-maxuv-l 8000 \
-auto-threshold 3 \
-weight-briggs -0.5 \
-niter 100000 \
-weighting-rank-filter 3.0 \
-clean-border 0 \
-fit-beam \
-apply-primary-beam \
-channel-division-frequencies 1.37e8,1.6e8 \
-channels-out 1
-reorder \
-update-model-required \
-name midf_wholetime_noslice \
-size 2048 2048
-scale 1asec \
*.ms
#+END_src


* Struis

Struis - Amsterdam HPC system. You'll need to acquire login details for this.
~ssh <username>@struis.science.uva.nl~

[[file:login-info.org::* Struis SSH][Login details]]


* TRAP

Software for analysing LOFAR data.

For Python3 - setup is easier (I've saved the word to Documents somewhere but requires asking antonia for a python3 database I believe)

** Installation -- for tkp4.0 / python2.7 version

Clone latest version from Github.
~git clone https://github.com/transientskp/tkp.git~

Create a virtualenv if you don't already have one and source it. TraP 5.0 runs on Python2 still so ensure virtualenv is setup accordingly.
#+begin_src zsh
virtualenv trap_env_2023 --python=python2.7


conda activate
#+end_src

Install Jupyter notebook.
#+begin_src bash
pip install --upgrade pip
pip install notebook
#+end_src

Install boost
#+begin_src bash
conda install -c conda-forge boost
#+end_src

*Changing TraP version*
Install tkp with developer mode with the '-e' tag, meaning we can use the Git checkout feature.
#+begin_src zsh
cd ~/tkp
pip install -e ".[pixelstore]"
git tag             # Shows all available tags.
git checkout r5.0   # 5.0 is the python 2.7 version I think current banana uses
#+end_src

*** TODO change commands to conda create env commands

** Setting up TRAP

Ensure you're the virtual environment you setup.

#+begin_src shell
# initialise a new project (only do once?)
trap-manage.py initproject promptradio

# create a new database
creatdb -h vlo.science.uva.nl -U ahennessey <databasename>
<postgresql password>

# edit pipeline.cfg

# initialise the databse with tkp
trap-manage.py initdb
#+end_src

*** TKP PostgreSQL Login Details
~ahennessey~
~5tF69ShycX~


** Using TRAP

#+begin_src shell
# initialise the job
trap-manage.py initjob <jobname>

# edit config files
./<jobnames>/job_params.cfg # job parameters
./<jobnames>/images_to_process.py # point to image files

# run the pipeline
trap-manage.py run <jobname>

# for monitoring a position, can supply specific coordinates
trap-manage.py run [-m MONITOR_COORDS] [-l COORDS_FILE] <jobname>

# MONITOR COORDS -> a list of ra, dec coordinates in JSON format (decima degrees)
#                -> [[203.234, 120.234], [123.704, 090.234]]
# COORDS_FILE -> specify a file containing a json formatted list of coordinates as above

# output to logfile rather than terminal and track it
nohup trap-manage.py run <jobname> > trap_output.log &
tail -f trap_output.log
#end_src


* PostgreSQL

General information and commands for PostgreSQL can be found at: [[file:software.org::*PostgreSQL][software.org/postgresql]]

LOFAR specific useful commands:
#+begin_src shell
# access psql terminal
psql -U ahennessey -h vlo.science.uva.nl -d <databasename>

# export database table to csv file
\copy extractedsource TO '/scratch/ahennessey/extract_240414a.csv' CSV HEADER;
#+end_src


** Deleting databases
Access the database as above, then after using ~\dt~ to list tables, you can use ~DELETE FROM table;~ to remove each table. You will find a series of linked foreign keys, just delete the table that it's referencing from one by one until all is gone.

[[~/org/guides/software.org::* Solving foreign key issues][Foreign key issues!]]


* Banana

Used for viewing the ran files from TRAP. This doesn't automatically work for newest LINC pipelines now due to using Python 3. Instead you must manually use SQL to download as csv files and process yourself.

[[file:login-info.org::* Banana][Login details]]
